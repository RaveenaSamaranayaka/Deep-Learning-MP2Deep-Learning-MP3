{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **English To Sinhala Translation**"
      ],
      "metadata": {
        "id": "mOvap2imVNuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Relavent Libraries"
      ],
      "metadata": {
        "id": "N02LMIqjVW1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H0_MJaVfJoaH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Masking\n",
        "from keras.layers import Attention, LayerNormalization, Dropout\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the google drive"
      ],
      "metadata": {
        "id": "u2vefdtHaT2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcWB_iOcaWoi",
        "outputId": "5cc9aa49-dddb-4849-f9c0-349b1dc21d32"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read The data file"
      ],
      "metadata": {
        "id": "kxyqcsVgVjQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/My Drive/DL_mini_project3/Trainingtxt.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHq-fZFVl2v",
        "outputId": "10740de7-435e-4b58-c867-79cafc1d1a04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\tyou will receive a package in the mail\n",
            "ලියාපදිංචිය සම්පූර්ණ කලාට පස්සෙ ඔයාට තහවුරු කිරීමේ කේතයක්  හම්බවේවි\tyou will receive a confirmation code after completing the registration\n",
            "ඊලඟ මිලදී ගැනීම කරන කොට ඔයාට වට්ටමක් හම්බවේවි\tyou will receive a discount on your next purchase\n",
            "ඔයාට අපේ පාරිභෝගික සේවා කණ්ඩායමෙන් දුරකථන ඇමතුමක් හම්බවේවි\tyou will receive a phone call from our customer service team\n",
            "ඔයාගේ ඇණවුම සූදානම් උනාට පස්සෙ ඔයාට දැනුම් දීමක් හම්බවේවි\tyou will receive a notification when your order is ready for pickup\n",
            "ඔයාට පැය 24ක් ඇතුලත ඔයාගෙ විමසීමට පිළිතුරක්  හම්බවේවි\tyou will receive a response to your inquiry within 24 hours\n",
            "ඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\tyou will receive a gift for your loyalty\n",
            "ඔයාට උත්සවයට ආරාධනාවක් හම්බවේවි\tyou will receive an invitation to the event\n",
            "ඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\tyou will receive a refund for the returned item\n",
            "ඔයාට විස්තර එක්ක  ඊමේල් එකක් හම්බවේවි\tyou'll receive an email with the details\n",
            "ඔයාට තැපෑලෙන් පැකේජ් එකක් හම්බවේවි\tyou'll receive a package in the mail\n",
            "ඔයාට රිටන් කරන භාණ්ඩය වෙනුවෙන් ආපසු මුදල් ගෙවීමක් හම්බවේවි\tyou'll receive a refund for the returned item\n",
            "ඔයාට ඔයාගේ පක්ෂපාතිත්වය වෙනුවෙන් තෑග්ගක් හම්බවේවි\tyou'll receive a gift for your loyalty\n",
            "ඔයාගෙ අම්මට කාර් එකක් හම්බවේවි\tyour mother will receive a car\n",
            "ඔයාගෙ සහෝදරයට කාර් එකක් හම්බවේවි\tyour brother will receive a car\n",
            "ඒක මාර ලස්සනයි\tit's pretty cool\n",
            "අපි ඔයාට ආරාධනා කරනවා ඒක කරන්න කියලා\twe're inviting you to do that\n",
            "යන්ත්‍ර පරිවර්තනය දියුණු කරන්න අපිත් එක්ක එකතු වෙන්න\tjoin us in improving machine translation\n",
            "ඔයාගෙ පරිවර්තන NMT මොඩලය පුහුණු කරන්න පාවිච්චි කරාවි\tyour translations will be used to train an nmt model\n",
            "අපි ඔයාට ආරාධනා කරනවා  ZoomNMT පුහුණු කරන්න පරිවර්තන සාම්පල ඉදිරිපත් කරන්න කියලා\twe're inviting you to submit translation samples for the zoomnmt training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the English and Sinhala translation pairs"
      ],
      "metadata": {
        "id": "7WnkBEmya4Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import re\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "\n",
        "for line in lines:\n",
        "    if '\\t' in line:\n",
        "        english, sinhala = line.split(\"\\t\")\n",
        "        sinhala = \"[start]\" + sinhala.strip() + \"[end]\"\n",
        "        text_pairs.append((english.strip(), sinhala))\n",
        "for i in range(3):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z1bu3TDb83M",
        "outputId": "4f3f30a6-8d1f-499d-a605-0a27b7ec715e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('හරි', '[start]ok[end]')\n",
            "('ලෙඩක් හරියටම හොයාගන්න පුලුවන්', '[start]a disease can be found exactly[end]')\n",
            "('කවුරුහරි ලුනු බැරල් එකක් වතුරට හලලා', '[start]someone poured a barrel of salt into the water[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Print random text pairs\n",
        "for i in range(3):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkax9GKRc7Dj",
        "outputId": "2811b803-1948-478f-b595-a0d3e20f4d5d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('හ්ම් ඔව්', '[start]hmm yes[end]')\n",
            "('මං ඊලඟ දවස එනකල් බලාන ඉන්නවා', '[start]i am waiting for the next day[end]')\n",
            "('මාර්වින්     තාත්තා', '[start]marvin    father[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomize the Data"
      ],
      "metadata": {
        "id": "J17Fmkyhc_7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "qhEemBXodor9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting the data into training, validation and Testing"
      ],
      "metadata": {
        "id": "NyaC7mSxdwJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIhGl8ROd1i8",
        "outputId": "5fcd6dc3-9ee5-4226-92bb-a44504bbcc14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 54213\n",
            "Training set size: 37951\n",
            "Validation set size: 8131\n",
            "Testing set size: 8131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKu84jbluoa2",
        "outputId": "d7aee6eb-808e-4484-9159-1747789b475d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54213"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Puctuation"
      ],
      "metadata": {
        "id": "GqzZWhN0ecF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1Dixh1fMeerV",
        "outputId": "a665f990-a271-45df-b5a6-b53e16953dfa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizing the English and Sinhala text pairs\n"
      ],
      "metadata": {
        "id": "ygUxUaGyfsM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(\n",
        "    lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "source_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length + 1,\n",
        "  standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_sinhala_texts = [pair[1] for pair in train_pairs]"
      ],
      "metadata": {
        "id": "5ioqIDw4ftKu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_sinhala_texts)"
      ],
      "metadata": {
        "id": "YFB3cF0UvDTD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare dataset for the translation task"
      ],
      "metadata": {
        "id": "3nIhAn18g4dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "\n",
        "def format_dataset(eng,sin):\n",
        "  eng=source_vectorization(eng)\n",
        "  sin=target_vectorization(sin)\n",
        "  return({\n",
        "      \"english\":eng,\n",
        "      \"sinhala\":sin[:,:-1],\n",
        "  },sin[:,1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, sin_texts =zip(*pairs)\n",
        "  eng_texts =list(eng_texts)\n",
        "  sin_texts =list(sin_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, sin_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape:{inputs['english'].shape}\")\n",
        "  print(f\"inputs['sinhala'].shape:{inputs['sinhala'].shape}\")\n",
        "  print(f\"targets.shape:{targets.shape}\")\n",
        "\n",
        "  inputs['english'].shape:  (64, 20)\n",
        "  inputs['sinhala'].shape: (64, 20)\n",
        "  targets.shape: (64, 20)\n",
        "print(list(train_ds.as_numpy_iterator())[50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tn_E3p5g720",
        "outputId": "20daea3c-5ede-4b2f-e56c-4fcb0c9330ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape:(64, 20)\n",
            "inputs['sinhala'].shape:(64, 20)\n",
            "targets.shape:(64, 20)\n",
            "({'english': array([[   5,  374,    3, ...,    0,    0,    0],\n",
            "       [   6,  713,    0, ...,    0,    0,    0],\n",
            "       [  56,   50,  414, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [4878,   55,  255, ...,    0,    0,    0],\n",
            "       [  84,    0,    0, ...,    0,    0,    0],\n",
            "       [ 921,  322,  257, ...,    0,    0,    0]]), 'sinhala': array([[   7,   67,   33, ...,    0,    0,    0],\n",
            "       [  48,    6,    3, ...,    0,    0,    0],\n",
            "       [  35, 2503,   39, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 594,   14,    5, ...,    0,    0,    0],\n",
            "       [ 159,   25,    0, ...,    0,    0,    0],\n",
            "       [ 106,    3, 1262, ...,    0,    0,    0]])}, array([[  67,   33,  198, ...,    0,    0,    0],\n",
            "       [   6,    3, 1439, ...,    0,    0,    0],\n",
            "       [2503,   39,  108, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  14,    5, 6604, ...,    0,    0,    0],\n",
            "       [  25,    0,    0, ...,    0,    0,    0],\n",
            "       [   3, 1262,  262, ...,    0,    0,    0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "9j_y_MohicmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "I6_h5foQhAuW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer decoder"
      ],
      "metadata": {
        "id": "uBGodK_jis4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask\n",
        "        )\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "a43ZQs0uiv08"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding"
      ],
      "metadata": {
        "id": "rA8TgPkFjetl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "UoPbCzL3i17E"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "End To End Transformer"
      ],
      "metadata": {
        "id": "vcwwliLljiyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "\n",
        "\n",
        "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4HnQgZ_jmb7",
        "outputId": "21ae54f9-8922-4263-dfa6-a2ff0aedc6a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding (Posi  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " tionalEmbedding)                                                                                 \n",
            "                                                                                                  \n",
            " positional_embedding_1 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
            " formerEncoder)                                                                                   \n",
            "                                                                                                  \n",
            " transformer_decoder (Trans  (None, None, 256)            5259520   ['positional_embedding_1[0][0]\n",
            " formerDecoder)                                                     ',                            \n",
            "                                                                     'transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, None, 256)            0         ['transformer_decoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, None, 15000)          3855000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the sequence-sequence Transformer"
      ],
      "metadata": {
        "id": "mQrP-KFpj2x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(optimizer=\"rmsprop\",\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtIZZV8Aj786",
        "outputId": "8f4bbcdf-5853-4691-857f-8008d380a9a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "593/593 [==============================] - 57s 81ms/step - loss: 5.3386 - accuracy: 0.2698 - val_loss: 4.5007 - val_accuracy: 0.3387\n",
            "Epoch 2/30\n",
            "593/593 [==============================] - 39s 66ms/step - loss: 4.4600 - accuracy: 0.3544 - val_loss: 4.1032 - val_accuracy: 0.3892\n",
            "Epoch 3/30\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 4.0762 - accuracy: 0.4004 - val_loss: 3.9152 - val_accuracy: 0.4135\n",
            "Epoch 4/30\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 3.7808 - accuracy: 0.4374 - val_loss: 3.8346 - val_accuracy: 0.4267\n",
            "Epoch 5/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 3.5358 - accuracy: 0.4707 - val_loss: 3.7798 - val_accuracy: 0.4385\n",
            "Epoch 6/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 3.3281 - accuracy: 0.4996 - val_loss: 3.7920 - val_accuracy: 0.4467\n",
            "Epoch 7/30\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 3.1611 - accuracy: 0.5225 - val_loss: 3.8204 - val_accuracy: 0.4496\n",
            "Epoch 8/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 3.0139 - accuracy: 0.5449 - val_loss: 3.8600 - val_accuracy: 0.4559\n",
            "Epoch 9/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.8989 - accuracy: 0.5628 - val_loss: 3.8680 - val_accuracy: 0.4583\n",
            "Epoch 10/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.7990 - accuracy: 0.5803 - val_loss: 3.9216 - val_accuracy: 0.4508\n",
            "Epoch 11/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.7165 - accuracy: 0.5950 - val_loss: 3.9325 - val_accuracy: 0.4599\n",
            "Epoch 12/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.6486 - accuracy: 0.6064 - val_loss: 3.9739 - val_accuracy: 0.4561\n",
            "Epoch 13/30\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 2.5867 - accuracy: 0.6193 - val_loss: 4.0220 - val_accuracy: 0.4624\n",
            "Epoch 14/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.5420 - accuracy: 0.6289 - val_loss: 4.0499 - val_accuracy: 0.4622\n",
            "Epoch 15/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.4950 - accuracy: 0.6374 - val_loss: 4.0684 - val_accuracy: 0.4629\n",
            "Epoch 16/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.4578 - accuracy: 0.6459 - val_loss: 4.1329 - val_accuracy: 0.4607\n",
            "Epoch 17/30\n",
            "593/593 [==============================] - 41s 70ms/step - loss: 2.4180 - accuracy: 0.6541 - val_loss: 4.1781 - val_accuracy: 0.4639\n",
            "Epoch 18/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.3822 - accuracy: 0.6619 - val_loss: 4.2085 - val_accuracy: 0.4618\n",
            "Epoch 19/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.3571 - accuracy: 0.6659 - val_loss: 4.2252 - val_accuracy: 0.4633\n",
            "Epoch 20/30\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 2.3245 - accuracy: 0.6731 - val_loss: 4.2336 - val_accuracy: 0.4638\n",
            "Epoch 21/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.2972 - accuracy: 0.6784 - val_loss: 4.2711 - val_accuracy: 0.4642\n",
            "Epoch 22/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.2828 - accuracy: 0.6821 - val_loss: 4.2836 - val_accuracy: 0.4600\n",
            "Epoch 23/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.2522 - accuracy: 0.6875 - val_loss: 4.2670 - val_accuracy: 0.4670\n",
            "Epoch 24/30\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 2.2334 - accuracy: 0.6910 - val_loss: 4.2857 - val_accuracy: 0.4671\n",
            "Epoch 25/30\n",
            "593/593 [==============================] - 40s 68ms/step - loss: 2.2135 - accuracy: 0.6949 - val_loss: 4.3056 - val_accuracy: 0.4683\n",
            "Epoch 26/30\n",
            "593/593 [==============================] - 41s 68ms/step - loss: 2.1904 - accuracy: 0.6993 - val_loss: 4.3797 - val_accuracy: 0.4639\n",
            "Epoch 27/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.1786 - accuracy: 0.7025 - val_loss: 4.3569 - val_accuracy: 0.4703\n",
            "Epoch 28/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.1549 - accuracy: 0.7067 - val_loss: 4.3952 - val_accuracy: 0.4699\n",
            "Epoch 29/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.1425 - accuracy: 0.7090 - val_loss: 4.3950 - val_accuracy: 0.4693\n",
            "Epoch 30/30\n",
            "593/593 [==============================] - 41s 69ms/step - loss: 2.1215 - accuracy: 0.7138 - val_loss: 4.4285 - val_accuracy: 0.4689\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b08374859f0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the vocabulary and index lookup for Sinhala\n",
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(\"English: \", input_sentence)\n",
        "    print(\"Sinhala: \", decode_sequence(input_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKxaLgWkyyuy",
        "outputId": "71e3877f-c900-4a4a-a1d6-3abee67c6fd5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "English:  මම එක බෑග් එකක් අරගත්තා\n",
            "Sinhala:  [start] had one like one[end]                \n",
            "-\n",
            "English:  මෙච්චර වෙලා උනාපහල තාම සුද්ධ කරල ඉවර නෑනේ\n",
            "Sinhala:  [start] from not so there was done yet[end]             \n",
            "-\n",
            "English:  මම හිතුවේ ඔයා ආයේ මාව හම්බවෙන්නේ නෑ කියලා\n",
            "Sinhala:  [start] thought i would not think like you again[end]        me[end] me[end]   \n",
            "-\n",
            "English:  ඒත් මට ඕනේ වැඩිහිටි එකක් කරන්න\n",
            "Sinhala:  [start] i want to make a got an great die[end]           \n",
            "-\n",
            "English:  මේ ගෙදර තියෙනවා පරණ ලස්සන දේවල් ටොන් ගානක්\n",
            "Sinhala:  [start] from new things in this is a beautiful of father[end]          \n",
            "-\n",
            "English:  මං මගේ තත්වේ පෙන්වන එක නතර කරන්නම්\n",
            "Sinhala:  [start] i come on my chance[end]               \n",
            "-\n",
            "English:  ඒවගේම රුසියාව එක්ක තියෙන රාජ්‍යත්‍රාන්ත්‍රික සම්බන්ධතා නිසා\n",
            "Sinhala:  [start] from things with kehelmal got into the village give away[end]          \n",
            "-\n",
            "English:  මට තියෙන්නේ මෙච්චරයි\n",
            "Sinhala:  [start] there is no love me[end]               \n",
            "-\n",
            "English:  ඔයා කොහේවත් යන් නැහැ\n",
            "Sinhala:  [start] not be together[end]                 \n",
            "-\n",
            "English:  ඔයා අපිට බියර් එකක් ණයයි\n",
            "Sinhala:  [start] change a change to sleep[end]               \n",
            "-\n",
            "English:  අපේ නගරේ ඉස්පිරිතාලෙ ඇයි ඉපදෙන්නෙ\n",
            "Sinhala:  [start] from city not to stay at our city[end]            \n",
            "-\n",
            "English:  අපේ සතුටු පුංචි කල්ලියට අඳුරු ඉරක් ගෙනාවා\n",
            "Sinhala:  [start] very happy with little sister was taken the eyes[end]           \n",
            "-\n",
            "English:  ෆීනික්ස් කුරුල්ලා හරි ඔයාලා එකිනෙකා වෙනුවෙන් ඉන්නේ\n",
            "Sinhala:  [start] on my sister on you are for each other[end]           \n",
            "-\n",
            "English:  බිත්තර එලියට ගත්තෙ ඇයි\n",
            "Sinhala:  [start] was out why didnt eat[end]               \n",
            "-\n",
            "English:  සර්  මොකද\n",
            "Sinhala:  [start] what[end]                   \n",
            "-\n",
            "English:  මම හිතන්නේ එයාලා ගිහින්\n",
            "Sinhala:  [start] think go to the leave[end]               \n",
            "-\n",
            "English:  මොකක්\n",
            "Sinhala:  [start] what[end]                   \n",
            "-\n",
            "English:  මොකක් මොකක්ද\n",
            "Sinhala:  [start] what[end]                   \n",
            "-\n",
            "English:  මම හිතුවේ එයා මැරිලා කියලා\n",
            "Sinhala:  [start] thought he was dead[end]                \n",
            "-\n",
            "English:  නෑ ඒක එතනම තිබ්බාවේ\n",
            "Sinhala:  [start] thats not sleep[end]                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sin_vocab = target_vectorization.get_vocabulary()\n",
        "sin_index_lookup = dict(zip(range(len(sin_vocab)), sin_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = sin_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Get English input from the user\n",
        "input_sentence = input(\"Enter a sinhala sentence: \")\n",
        "\n",
        "# Translate the input sentence to Sinhala\n",
        "translated_sentence = decode_sequence(input_sentence)\n",
        "\n",
        "# Print the translated sentence\n",
        "print(\"English translation:\", translated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueRx6D7vVjcr",
        "outputId": "497a8e62-5fe7-42db-fe09-126e2ee87db8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sinhala sentence: මම ගෙදර යනවා \n",
            "English translation: [start] i go home[end]                 \n"
          ]
        }
      ]
    }
  ]
}